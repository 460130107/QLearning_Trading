{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Capstone Project\n",
    "### Learning to Trade Using Q-Learning\n",
    "<sub>Uirá Caiado. Aug 10, 2016<sub>\n",
    "\n",
    "\n",
    "#### Abstract\n",
    "\n",
    "*In this project, I will present an adaptive learning model to trade a single stock under the reinforcement learning framework. This area of machine learning consists in training an agent by reward and punishment without needing to specify the expected action. The agent learns from its experience and develops a strategy that maximizes its profits. \"The simulation results show initial success in bringing learning techniques to build algorithmic trading strategies\".[to be checked]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In this section, I will provide a high-level overview of the project, define the problem addressed and the metric used to measure the performance of the model created.\n",
    "\n",
    "### 1.1. Project Overview\n",
    "```\n",
    "Udacity:\n",
    "\n",
    "In this section, look to provide a high-level overview of the project in layman’s terms. Questions to ask yourself when writing this section:\n",
    "- Has an overview of the project been provided, such as the problem domain, project origin, and related datasets or input data?\n",
    "- Has enough background information been given so that an uninformed reader would understand the problem domain and following problem statement?\n",
    "```\n",
    "Nowadays, algo trading represents almost half of all cash equity trading in western Europe. In advanced markets, it already [accounts](http://en.resenhadabolsa.com.br/portfolio-category/the-distributionintermediation-industry-in-brazil-challenges-and-trends/) for over 40%-50% of total volume. In Brazil its market share is not as large – currently about 10% – but is expected to rise in the years ahead as markets and players go digital.\n",
    "\n",
    "As automated strategies are becoming increasingly popular, building an intelligent system that can trade many times a day and adapts itself to the market conditions and still consistently makes money is a subject of keen interest of any market participant.\n",
    "\n",
    "Given that it is hard to produce such strategy, in this project I will try to build an algorithm that just does better than a random agent, but learns by itself how to trade. To do so, I will feed my agent with one month of information about every trade and change in the [top of the order book](https://goo.gl/k1dDYZ) in the [PETR4](https://pt.wikipedia.org/wiki/Petrobras) - one of the most liquidity assets in Brazilian Stock Market - in a Reinforcement Learning Framework. Later on, I will test what it has learned in a newest dataset.\n",
    "\n",
    "The dataset used in this project is also known as [level I order book data](https://www.thebalance.com/order-book-level-2-market-data-and-depth-of-market-1031118) and includes all trades and changes in the prices and total quantities at best Bid (those who wants to buy the stock) and Offer side (those who intends to sell the stock)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2. Problem Statement\n",
    "```\n",
    "Udacity:\n",
    "\n",
    "In this section, you will want to clearly define the problem that you are trying to solve, including the strategy (outline of tasks) you will use to achieve the desired solution. You should also thoroughly discuss what the intended solution will be for this problem. Questions to ask yourself when writing this section:\n",
    "- Is the problem statement clearly defined? Will the reader understand what you are expecting to solve?\n",
    "- Have you thoroughly discussed how you will attempt to solve the problem?\n",
    "- Is an anticipated solution clearly defined? Will the reader understand what results you are looking for?\n",
    "```\n",
    "[Algo trading](http://goo.gl/b9jAqE) strategies usually are programs that follow a predefined set of instructions to place its orders. The primary challenge to this approach is building these rules in a way that it can consistently generate profit without being too sensitive to market conditions. \n",
    "\n",
    "Thus, the goal of this project is to develop an adaptative learning model that can learn by itself those rules and trade a particular asset using reinforcement learning framework under an environment that replays historical high-frequency data.\n",
    "\n",
    "As \\cite{chan2001electronic} describe, reinforcement learning can be considered as a model-free approximation of dynamic programming. The knowledge of the underlying processes is not assumed but learned from experience.\n",
    "\n",
    "The agent can access some information about the environment state as the order flow imbalance, the sizes of the best bid and offer and so on. At each time step $t$, It should generate some valid action, as buy stocks or insert a limit order at the Ask side. All inputs and actions will be detailed in the next sections.\n",
    "\n",
    "The agent also should receive a reward or a penalty at each time step if it is already carrying a position from previous rounds or if it has made a trade (the cost of the operations are computed as a penalty).\n",
    "\n",
    "Based on the rewards and penalties it gets, the agent should learn an optimal policy for trade this particular stock, maximizing the profit it receives from its actions and resulting positions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3. Metrics\n",
    "```\n",
    "Udacity:\n",
    "\n",
    "In this section, you will need to clearly define the metrics or calculations you will use to measure performance of a model or result in your project. These calculations and metrics should be justified based on the characteristics of the problem and problem domain. Questions to ask yourself when writing this section:\n",
    "- Are the metrics you’ve chosen to measure the performance of your models clearly discussed and defined?\n",
    "- Have you provided reasonable justification for the metrics chosen based on the problem and solution?\n",
    "```\n",
    "\n",
    "In 1988, the Wall Street Journal created a [Dartboard Contest](http://www.automaticfinances.com/monkey-stock-picking/), where Journal staffers threw darts at a stock table to select their assets, while investment experts picked their own stocks. After six months, they compared the results of the two methods. After adjusting the results to risk level, they found out that the pros barely have beaten the random pickers.\n",
    "\n",
    "Given that, the metric used to measure the performance of the learner will be the amount of money made by a random agent. So, my goal will be to outperform this agent, that should just produce some random action from a set of allowed action at each time $t$. In the next section, I will detail the behavior of this agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis\n",
    "\n",
    "In this section, I will explore the data set that will be used in the simulation, define and justify the inputs employed in the state representation of the algorithm, explain the reinforcement learning techniques used and provide a benchmark.\n",
    "\n",
    "### 2.1. Data Exploration\n",
    "```\n",
    "Udacity:\n",
    "\n",
    "In this section, you will be expected to analyze the data you are using for the problem. This data can either be in the form of a dataset (or datasets), input data (or input files), or even an environment. The type of data should be thoroughly described and, if possible, have basic statistics and information presented (such as discussion of input features or defining characteristics about the input or environment). Any abnormalities or interesting qualities about the data that may need to be addressed have been identified (such as features that need to be transformed or the possibility of outliers). Questions to ask yourself when writing this section:\n",
    "- If a dataset is present for this problem, have you thoroughly discussed certain features about the dataset? Has a data sample been provided to the reader?\n",
    "- If a dataset is present for this problem, are statistics about the dataset calculated and reported? Have any relevant results from this calculation been discussed?\n",
    "- If a dataset is **not** present for this problem, has discussion been made about the input space or input data for your problem?\n",
    "- Are there any abnormalities or characteristics about the input space or dataset that need to be addressed? (categorical variables, missing values, outliers, etc.)\n",
    "```\n",
    "The dataset used is composed by level I order book data from PETR4, a stock traded at BMFBovespa Stock Exchange. Includes 19 trading sessions from 07/25/2016 to 08/18/2016. I will use one day to create the scalers of the features used, that I shall explain. Then, I will use 16 days to train the model and finally, I will test the policy found in the last two days. The data was collected from Bloomberg. *Let's start by looking at the size of these files:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20160725.csv:\t110,756 rows\t4.42 MB\n",
      "20160726.csv:\t100,109 rows\t3.98 MB\n",
      "20160727.csv:\t123,175 rows\t4.93 MB\n",
      "20160728.csv:\t109,655 rows\t4.37 MB\n",
      "20160729.csv:\t135,111 rows\t5.40 MB\n",
      "20160801.csv:\t109,710 rows\t4.37 MB\n",
      "20160802.csv:\t108,053 rows\t4.30 MB\n",
      "20160803.csv:\t137,039 rows\t5.49 MB\n",
      "20160804.csv:\t139,118 rows\t5.56 MB\n",
      "20160805.csv:\t112,852 rows\t4.51 MB\n",
      "20160808.csv:\t89,730 rows\t3.55 MB\n",
      "20160809.csv:\t83,826 rows\t3.33 MB\n",
      "20160810.csv:\t105,758 rows\t4.21 MB\n",
      "20160811.csv:\t144,728 rows\t5.81 MB\n",
      "20160812.csv:\t147,086 rows\t5.90 MB\n",
      "20160815.csv:\t108,633 rows\t4.33 MB\n",
      "20160816.csv:\t108,795 rows\t4.33 MB\n",
      "20160817.csv:\t118,980 rows\t4.75 MB\n",
      "20160818.csv:\t84,489 rows\t3.36 MB\n",
      "==========================================\n",
      "TOTAL\t\t19 files\t86.90 MB\n",
      "\t\t2,177,603 rows\n",
      "CPU times: user 7.36 s, sys: 55.9 ms, total: 7.41 s\n",
      "Wall time: 7.49 s\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "s_fname = \"data/petr4_0725_0818_2.zip\"\n",
    "archive = zipfile.ZipFile(s_fname, 'r')\n",
    "def foo():\n",
    "    f_total = 0.\n",
    "    f_tot_rows = 0.\n",
    "    for i, x in enumerate(archive.infolist()):\n",
    "        f_total += x.file_size/ 1024.**2\n",
    "        for num_rows, row in enumerate(archive.open(x)):\n",
    "            f_tot_rows += 1\n",
    "        print \"{}:\\t{:,.0f} rows\\t{:0.2f} MB\".format(x.filename, num_rows + 1, x.file_size/ 1024.**2)\n",
    "    print '=' * 42\n",
    "    print \"TOTAL\\t\\t{} files\\t{:0.2f} MB\".format(i+1,f_total)\n",
    "    print \"\\t\\t{:0,.0f} rows\".format(f_tot_rows)\n",
    "\n",
    "%time foo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 19 files, each one has 110,000 rows on average, resulting in 2,117,603 row at total and more than 86 MB of information. Now, let's look at the structure of one of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Type</th>\n",
       "      <th>Price</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-25 10:02:00</td>\n",
       "      <td>TRADE</td>\n",
       "      <td>11.98</td>\n",
       "      <td>5800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-25 10:02:00</td>\n",
       "      <td>BID</td>\n",
       "      <td>11.97</td>\n",
       "      <td>6100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-25 10:02:00</td>\n",
       "      <td>ASK</td>\n",
       "      <td>11.98</td>\n",
       "      <td>51800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-25 10:02:00</td>\n",
       "      <td>ASK</td>\n",
       "      <td>11.98</td>\n",
       "      <td>56800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-25 10:02:00</td>\n",
       "      <td>ASK</td>\n",
       "      <td>11.98</td>\n",
       "      <td>56900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date   Type  Price   Size\n",
       "0 2016-07-25 10:02:00  TRADE  11.98   5800\n",
       "1 2016-07-25 10:02:00    BID  11.97   6100\n",
       "2 2016-07-25 10:02:00    ASK  11.98  51800\n",
       "3 2016-07-25 10:02:00    ASK  11.98  56800\n",
       "4 2016-07-25 10:02:00    ASK  11.98  56900"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "l_fnames = archive.infolist()\n",
    "df = pd.read_csv(archive.open(l_fnames[0]), index_col=0, parse_dates=['Date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each file is composed of four different fields. The column $Date$ is the timestamp of the row and has a precision of seconds. $Type$ is the kind of information that the row encompasses. The type \"TRADE\" relates to an actual trade that has happened. \"BID\" is related to changes in the best Bid level and \"ASK,\" to the best Offer level. $Price$ is the current best bid or ask and $Size$ is the cumulated quantity on that price and side.\n",
    "\n",
    "All this data will be used to create the environment where my agent will operate. This environment is an order book, where the agent will be able to insert limit orders and execute trades at the best prices. The order book is represented by two binary trees, one for the Bid and other for the Ask side. As can be seen in the table below, the nodes of these trees are sorted by price (price level) in ascending order on the Bid side and descending order on the ask side. At each price level, there are other binary trees sorted by order of arrival. The first order to arrive is the first order filled when coming in a trade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.8 s, sys: 45.4 ms, total: 13.9 s\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "import qtrader.simulator as simulator\n",
    "import qtrader.environment as environment\n",
    "e = environment.Environment()\n",
    "sim = simulator.Simulator(e)\n",
    "%time sim.run(n_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qBid</th>\n",
       "      <th>Bid</th>\n",
       "      <th>Ask</th>\n",
       "      <th>qAsk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61,400</td>\n",
       "      <td>12.02</td>\n",
       "      <td>12.03</td>\n",
       "      <td>13,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47,100</td>\n",
       "      <td>12.01</td>\n",
       "      <td>12.04</td>\n",
       "      <td>78,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51,700</td>\n",
       "      <td>12.00</td>\n",
       "      <td>12.05</td>\n",
       "      <td>20,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37,900</td>\n",
       "      <td>11.99</td>\n",
       "      <td>12.06</td>\n",
       "      <td>23,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97,000</td>\n",
       "      <td>11.98</td>\n",
       "      <td>12.07</td>\n",
       "      <td>27,900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     qBid    Bid    Ask    qAsk\n",
       "0  61,400  12.02  12.03  13,800\n",
       "1  47,100  12.01  12.04  78,700\n",
       "2  51,700  12.00  12.05  20,400\n",
       "3  37,900  11.99  12.06  23,100\n",
       "4  97,000  11.98  12.07  27,900"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.env.get_order_book()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment will answer with the agent's current position and Profit and Loss (PnL) every time the agent executes a trade or has an order filled. The cost of the trade will be accounted as a penalty.\n",
    "\n",
    "As \\cite{chan2001electronic} explained, Markov decision processes (MDPs) are the most common model for reinforcement learning.  The MDP model of the environment consists, among other things, of a discrete set of states $S$ and a discrete set of actions $A$ the agent can take. About the action space $A$, there are six possibles actions:\n",
    "\n",
    "$$a_t \\in \\left (None,\\, buy,\\, sell,\\, best\\_bid,\\, best\\_ask,\\, best\\_both \\right)$$\n",
    "\n",
    "Where $None$ indicates that the agent shouldn't have any order in the market. $Buy$ and $Sell$ mean that the agent should execute a market order to buy or sell $100$ stocks (the size of an order will always be a hundred shares). $best\\_bid$ and $best\\_ask$ indicates that the agent should keep order at the best price just in the mentioned side and $best\\_both$, it should have orders at the best price in both sides.\n",
    "\n",
    "The current state of the environment will be represented by a set of variables representing the events that have occurred at the last seconds and other variables describing the last condition of the order book. So, I will use the following inputs to represent the environment state:\n",
    "\n",
    "- $log\\_return$ : float. The Log return of the stock in the last 10 seconds\n",
    "- $qOFI$ : integer. The net order flow at the bid and ask in the last 10 seconds\n",
    "- $qBid$ : Integer. The size of the current best Bid\n",
    "- $book\\_ratio$ : float. The Bid size over the Ask size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "```\n",
    "Udacity:\n",
    "\n",
    "Exploratory Visualization:\n",
    "\n",
    "In this section, you will need to provide some form of visualization that summarizes or extracts a relevant characteristic or feature about the data. The visualization should adequately support the data being used. Discuss why this visualization was chosen and how it is relevant. Questions to ask yourself when writing this section:\n",
    "- Have you visualized a relevant characteristic or feature about the dataset or input data?\n",
    "- Is the visualization thoroughly analyzed and discussed?\n",
    "- If a plot is provided, are the axes, title, and datum clearly defined?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the Ask is not used because it is indirectly used in the $book\\_ratio$. The $log\\_return$ is defined as $r_t = \\log \\left(  \\frac{P_t}{P_{t-1}}\\right)$, where $P_t$ is the current price and $P_{t-1}$ is the previous price. Concerning the measure of the Order Flow Imbalance (OFI), there are many ways to measure it. \\cite{cont2014price} argued the *order flow imbalance* is a measure of supply/demand imbalance and defines it as a sum of individual event contribution $e_n$ over time intervals $\\left[ t_{k-1}, \\; t_k \\right]$, such that:\n",
    "\n",
    "$$OFI_k = \\sum^{N(t_k)}_{n=N(t_{k-1})+1} e_n$$\n",
    "\n",
    "Where $N(t_k)$ and $N(t_{k-1}) + 1$ are index of the first and last event in the interval. The $e_n$ was defined by the authors as a measure of the contribution of the $n$-th event to the size of the bid and ask queues:\n",
    "\n",
    "$$e_n = \\mathbb{1}_{P_{n}^{B} \\geq P_{n-1}^{B}} q^{B}_{n} - \\mathbb{1}_{P_{n}^{B} \\leq P_{n-1}^{B}}  q^{B}_{n-1} - \\mathbb{1}_{P_{n}^{A} \\leq P_{n-1}^{A}} q^{A}_{n} + \\mathbb{1}_{P_{n}^{A} \\geq P_{n-1}^{A}}  q^{A}_{n-1}$$\n",
    "\n",
    "Where $q^{B}_{n}$ and $q^{A}_{n}$ are linked to the cumulated quantities at the best bid and ask in the time $n$. The subscript $n-1$ is related to the last observation. $\\mathbb{1}$ is an [indicator](https://en.wikipedia.org/wiki/Indicator_function) function. In the figure below is ploted the 10-second log-return of PETR4 against the contemporaneous OFI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.44 s, sys: 9.55 ms, total: 2.45 s\n",
      "Wall time: 2.45 s\n"
     ]
    }
   ],
   "source": [
    "import qtrader.eda as eda; reload(eda);\n",
    "s_fname = \"data/petr4_0725_0818_2.zip\"\n",
    "%time eda.test_ofi_indicator(s_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAE5CAYAAAByAzHwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114d03c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/ofi_petr.txt', sep='\\t')\n",
    "df.drop('TIME', axis=1, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "ax = sns.lmplot(x=\"OFI\", y=\"LOG_RET\", data=df, markers=[\"x\"], palette=\"Set2\", size=4, aspect=2.)\n",
    "ax.ax.set_title(u'Relation between the Log-return and the $OFI$\\n', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As described by \\cite{cont2014price} in a similar test, the figure suggests that order flow imbalance is a stronger driver of high-frequency price changes and this variable will be used to describe the current state of the order book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.2. Algorithms and Techniques\n",
    "```\n",
    "Udacity:\n",
    "\n",
    "In this section, you will need to discuss the algorithms and techniques you intend to use for solving the problem. You should justify the use of each one based on the characteristics of the problem and the problem domain. Questions to ask yourself when writing this section:\n",
    "- Are the algorithms you will use, including any default variables/parameters in the project clearly defined?\n",
    "- Are the techniques to be used thoroughly discussed and justified?\n",
    "- Is it made clear how the input data or datasets will be handled by the algorithms and techniques chosen?\n",
    "```\n",
    "Based on \\cite{cont2014price}, the algo trading might be conveniently modeled in the framework of reinforcement learning. According to \\cite{du1algorithm}, this framework adjusts the parameters of an agent to maximize the expected payoff or reward that is generated due to its actions. It learns a policy, the actions the agent must perform to achieve its best performance. This optimal policy is exactly what we are looking for when we are building an algo trading strategy.\n",
    "\n",
    "So, defining a general formulation of the reinforcement learning problem based on a Markov Decision Process (MDP), as proposed by \\cite{Mitchell}, the agent can perceive a set $S$ os distinct states of its environment and has a set $A$ of actions that it can perform. So, at each discrete time step $t$, the agent senses the current state $s_t$ and choose to take an action $a_t$. The environment responds by providing the agent a reward $r_t=r(s_t, a_t)$ and by producing the succeeding state $s_{t+1}=\\delta(s_t, a_t)$. The functions $r$ and $\\delta$ only depend on the current state and action (it is [memoryless](https://en.wikipedia.org/wiki/Markov_process)), are part of the environment and are not necessarily known to the agent.\n",
    "\n",
    "The task of the agent is to learn a policy $\\pi$ that maps each state to an action ($\\pi: S \\rightarrow A$), selecting its next action $a_t$ based solely on the current observed state $s_t$, that is $\\pi(s_t)=a_t$. The optimal policy, or control strategy, is the one that produces the greatest possible cumulative reward over time. So, stating that:\n",
    "\n",
    "$$V^{\\pi}(s_t)= r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+1} + ... = \\sum_{i=0}^{\\infty} \\gamma^{i} r_{t+i}$$\n",
    "\n",
    "Where $V^{\\pi}(s_t)$ is also called the discounted cumulative reward and it represents the cumulative value achieved by following an policy $\\pi$ from an initial state $s_t$ and $\\gamma \\in [0, 1]$ is a constant that determines the relative value of delayed versus immediate rewards. It is one of the\n",
    "\n",
    "If we set $\\gamma=0$, only immediate rewards is considered. As $\\gamma \\rightarrow 1$, future rewards are given greater emphasis relative to immediate reward. The optimal policy $\\pi^{*}$ that will maximizes $V^{\\pi}(s_t)$ for all states $s$ can be written as:\n",
    "\n",
    "$$\\pi^{*} = \\underset{\\pi}{\\arg \\max} \\, V^{\\pi} (s)\\,\\,\\,\\,\\,, \\,\\, \\forall s$$\n",
    "\n",
    "However, learning $\\pi^{*}: S \\rightarrow A$ directly is difficult because the available training data does not provide training examples of the form $(s, a)$. Instead, as \\cite{Mitchell} explained, the only available information is the sequence of immediate rewards $r(s_i, a_i)$ for $i=1,\\, 2,\\, 3,\\,...$\n",
    "\n",
    "So, as we are trying to maximize the cumulative rewards $V^{*}(s_t)$ for all states $s$, the agent should prefer $s_1$ over $s_2$ wherever $V^{*}(s_1) > V^{*}(s_2)$. Given that the agent must choose among actions and not states, and it isn't able to perfectly predict the immediate reward and immediate successor for every possible state-action transition, we also must learn $V^{*}$ indirectly.\n",
    "\n",
    "To solve that, we define a function $Q(s, \\, a)$ such that its value is the maximum discounted cumulative reward that can be achieved starting from state $s$ and applying action $a$ as the first action. So, we can write:\n",
    "\n",
    "$$Q(s, \\, a) = r(s, a) + \\gamma V^{*}(\\delta(s, a))$$\n",
    "\n",
    "As $\\delta(s, a)$ is the state resulting from applying action $a$ to state $s$ (the successor) chosen by following the optimal policy, $V^{*}$ is the cumulative value of the immediate successor state discounted by a factor $\\gamma$. Thus,  what we are trying to achieve is\n",
    "\n",
    "$$\\pi^{*}(s) = \\underset{a}{\\arg \\max} Q(s, \\, a)$$\n",
    "\n",
    "Thus, the optimal policy can be obtained even if the agent just uses the current action $a$ and state $s$ and chooses the action that maximizes $Q(s,\\, a)$. Also, it is important to notice that the function above implies that the agent can select optimal actions even when it has no knowledge of the functions $r$ and $\\delta$.\n",
    "\n",
    "Lastly, according to \\cite{Mitchell}, there are some conditions to ensure that the reinforcement learning converges toward an optimal policy. On a deterministic MDP, the agent must select actions in a way that it visits every possible state-action pair infinitely often.\n",
    "\n",
    "As the most inputs suggested in the last subsection was defined in an infinite space, in the section 3 I will discretize those numbers before use them to train my agent. We also will see how \\cite{Mitchell} defined a reliable way to estimate training values for $Q$, given only a sequence of immediate rewards $r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.3. Benchmark\n",
    "```\n",
    "Udacity:\n",
    "\n",
    "In this section, you will need to provide a clearly defined benchmark result or threshold for comparing across performances obtained by your solution. The reasoning behind the benchmark (in the case where it is not an established result) should be discussed. Questions to ask yourself when writing this section:\n",
    "- Has some result or value been provided that acts as a benchmark for measuring performance?\n",
    "- Is it clear how this result or value was obtained (whether by data or by hypothesis)?\n",
    "```\n",
    "\n",
    "As described before, the random agent should select a random action form a set of valid action at each time step $t$. The set of action can change because the random agent is also limited to hold an open position of $400$ stocks at most, on any side. When it reaches its limit, it will be allowed just to perform actions that decrease its position. So, for instance, if it already [long](http://www.investopedia.com/terms/l/long.asp) in $400$ shares, the possible moves would be $\\left (None,\\, sell,\\, best\\_ask \\right)$. If it is [short](http://www.investopedia.com/terms/s/short.asp), it just can perfrom $\\left (None,\\, buy,\\, best\\_bid\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methodology\n",
    "_(approx. 3-5 pages)_\n",
    "\n",
    "### 3.1 Data Preprocessing\n",
    "```\n",
    "Udacity:\n",
    "\n",
    "In this section, all of your preprocessing steps will need to be clearly documented, if any were necessary. From the previous section, any of the abnormalities or characteristics that you identified about the dataset will be addressed and corrected here. Questions to ask yourself when writing this section:\n",
    "- If the algorithms chosen require preprocessing steps like feature selection or feature transformations, have they been properly documented?\n",
    "- Based on the **Data Exploration** section, if there were abnormalities or characteristics that needed to be addressed, have they been properly corrected?\n",
    "- If no preprocessing is needed, has it been made clear why?\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\cite{Mohri_2012} explained that reinforcement learning is the study of planning and learning in a scenario where a learner (or agent) actively interacts with the environment to achieve a particular goal. The achievement of the agent's objective is typically measured by the reward he receives from the environment and which he seeks to maximize. Therefore, as \\cite{chan2001electronic} described, the knowledge of the underlying process is not assumed but learned from experience.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    " the number of ticks that the mid-price has changed\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "The learner should receive the inputs mentioned above at each time step $t$, and generate one of the following six possible actions: insert an order (or keep) just at the **best_bid price**, just at the **best_ask** price, on both sides (**best_both**) or cancel all its orders (**None**). It also can **buy** (take the ask, in the market jargon), or **sell** (hit the bid).\n",
    "\n",
    "....\n",
    "\n",
    "Each learning session will include data from the largest part of a trading session, starting at 10:30 and closing at 16:30. Also, the agent will be allowed to hold a position of just 400 shares at maximum (buy or sell). When the learning session is over, all positions from the learner will be closed out so the agent always will start a new session without carrying stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2. Implementation\n",
    "\n",
    "```\n",
    "Udacity:\n",
    "\n",
    "In this section, the process for which metrics, algorithms, and techniques that you implemented for the given data will need to be clearly documented. It should be abundantly clear how the implementation was carried out, and discussion should be made regarding any complications that occurred during this process. Questions to ask yourself when writing this section:\n",
    "- Is it made clear how the algorithms and techniques were implemented with the given datasets or input data?\n",
    "- Were there any complications with the original metrics or techniques that required changing prior to acquiring a solution?\n",
    "- Was there any part of the coding process (e.g., writing complicated functions) that should be documented?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. The Q-Function\n",
    "\n",
    "As mention before, to learn an optimal policy $\\pi^{*}$, I can't learn a function $\\pi^{*}: S \\rightarrow A$ that maps a state to the optimal action directly. There is no such information upfront to be used as training data. Instead, as \\cite{Mitchell} explained, the only available information is the sequence of immediate rewards $r(s_i, a_i)$ for $i=1,\\, 2,\\, 3,\\,...$\n",
    "\n",
    "So, as we are trying to maximize the cumulative rewards $V^{*}(s_t)$ for all states $s$, the agent should prefer $s_1$ over $s_2$ wherever $V^{*}(s_1) > V^{*}(s_2)$. Given that the agent must choose among actions and not states, and it isn't able to perfectly predict the immediate reward and immediate successor for every possible state-action transition, we also must learn $V^{*}$ indirectly.\n",
    "\n",
    "To solve that, we define a function $Q(s, \\, a)$ such that its value is the maximum discounted cumulative reward that can be achieved starting from state $s$ and applying action $a$ as the first action. So, we can write:\n",
    "\n",
    "$$Q(s, \\, a) = r(s, a) + \\gamma V^{*}(\\delta(s, a))$$\n",
    "\n",
    "As $\\delta(s, a)$ is the state resulting from applying action $a$ to state $s$ (the successor) chosen by following the optimal policy, $V^{*}$ is the cumulative value of the immediate successor state discounted by a factor $\\gamma$. Thus,  what we are trying to achieve is\n",
    "\n",
    "$$\\pi^{*}(s) = \\underset{a}{\\arg \\max} Q(s, \\, a)$$\n",
    "\n",
    "Thus, the optimal policy can be obtained even if the agent just uses the current action $a$ and state $s$ and chooses the action that maximizes $Q(s,\\, a)$. Also, it is important to notice that the function above implies that the agent can select optimal actions even when it has no knowledge of the functions $r$ and $\\delta$. In the next subsection, we will see how \\cite{Mitchell} defined a reliable way to estimate training values for $Q$, given only a sequence of immediate rewards $r$.\n",
    "\n",
    "\n",
    "#### 3.2.2. Learning $Q$\n",
    "\n",
    "As we have seen, learning the Q function corresponds to learning the optimal policy. According to \\cite{Mohri_2012}, the optimal state-action value function $Q^{*}$ is defined for all $(s, \\, a) \\in S \\times A$ as the expected return for taking the action $a \\in A$ at the state $s \\in S$, following the optimal policy. So, it can be written as \\cite{Mitchell} suggested:\n",
    "\n",
    "$$V^{*}(s) = \\underset{a'}{\\arg \\max} Q(s, \\, a')$$\n",
    "\n",
    "Using this relationship, we can write a recursive definition of Q function, such that:\n",
    "\n",
    "$$Q(s, \\, a) = r(s, a) + \\gamma \\, \\underset{a'}{\\max} \\, Q(\\delta(s,\\, a), \\, a')$$\n",
    "\n",
    "The recursive nature of the function above implies that our agent doesn't know the actual $Q$ function. It just can estimate $Q$, that we will refer as $\\hat{Q}$. It will represents is hypothesis $\\hat{Q}$ as a large table that attributes each pair $(s\\, , \\, a)$ to a value for $\\hat{Q}(s,\\, a)$ - the current hypothesis about the actual but unknown value $Q(s, \\, a)$. I will initialize this table with zeros, but it could be filled with random numbers, according to \\cite{Mitchell}. Still according to him, the agent repeatedly should observe its current state $s$ and do the following:\n",
    "- Observe the current state $s$ and:\n",
    "    - Choose some action $a$ and execute it\n",
    "    - Receive the immediate reward $r = r(s, a)$\n",
    "    - initialize the table entry $\\hat{Q}(s, \\, a)$ to zero if there is no entry $(s, \\, a)$\n",
    "    - Observe the new state $s' = \\delta(s, \\,a)$. \n",
    "    - Updates the table entry for $\\hat{Q}(s, \\, a)$ following:\n",
    "        - $\\hat{Q}(s, \\, a) \\leftarrow r + \\gamma \\underset{a'}{\\max} \\hat{Q}(s', \\, a')$\n",
    "- $s \\leftarrow s'$\n",
    "    \n",
    "Note this training rule is suited to a deterministic Markov decision process (where $r(s,\\, a)$ and $\\delta(s,\\, a)$ are deterministics) and uses the agent's current $\\hat{Q}$ values for the new state $s'$ to refine its estimate of $\\hat{Q}(s, \\,a)$ for the previous state $s$. It is tricky. So, let's take a look at its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.3. Refinement\n",
    "```\n",
    "Udacity:\n",
    "\n",
    "In this section, you will need to discuss the process of improvement you made upon the algorithms and techniques you used in your implementation. For example, adjusting parameters for certain models to acquire improved solutions would fall under the refinement category. Your initial and final solutions should be reported, as well as any significant intermediate results as necessary. Questions to ask yourself when writing this section:\n",
    "- Has an initial solution been found and clearly reported?\n",
    "- Is the process of improvement clearly documented, such as what techniques were used?\n",
    "- Are intermediate and final solutions clearly reported as the process is improved?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1. Experimentation Strategies\n",
    "\n",
    "One of the issues of the current strategy is that the agent could overcommit to actions that presented positive $\\hat{Q}$ values early in the simulation, failing to explore other actions that could present even higher values. \\cite{Mitchell} proposed to use a probabilistic approach to select actions, assigning higher probabilities to action with high $\\hat{Q}$ values, but given to every action at least a nonzero probability. So, I will implement the following relation:\n",
    "\n",
    "$$P(a_i\\, | \\,s ) = \\frac{k ^{\\hat{Q}(s, a_i)}}{\\sum_j k^{\\hat{Q}(s, a_j)}}$$\n",
    "\n",
    "Where $P(a_i\\, | \\,s )$ is the probability of selecting the action $a_i$ given the state $s$. The constant $k$ is positive and determines how strongly the selection favors action with high $\\hat{Q}$ values. Let's look at the number of times the algorithm choose to explore when we vary the value of the $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2. Nondeterministic Rewards and Actions\n",
    "\n",
    "The last modification that I want to introduce is when the environment is non-deterministic. As I said before, the algorithm used is appropriate to the deterministic case. On a nondeterministic environment, the reward function $r(s,\\,a)$ and action transition function $\\delta(s,\\,a)$ may have probabilistic outcomes. As there are other agents in the smartcab world, I believe that it is a proper assumption.\n",
    "\n",
    "As explained by \\cite{Mitchell}, to handle nondeterministic MDPs, it is needed to redefine the value $V^{*}$ to be expressed as a expected value. So, jumping to the $Q$ function, we can re-express $Q$ recursively as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "Q(s, \\, a) &= E \\left [r(s, a) \\right ] + E \\left [\\gamma \\, \\underset{a'}{\\max} \\, Q(\\delta(s,\\, a), \\, a')\\right ]\\\\\n",
    "&= E \\left [r(s, a) \\right ] + \\gamma \\,  E \\left [\\underset{a'}{\\max} \\, Q(\\delta(s,\\, a), \\, a')\\right ]\\\\\n",
    "&= E \\left [r(s, a) \\right ] + \\gamma \\,  \\sum_{s'} P\\left(s' \\,| \\,s, a   \\right) \\underset{a'}{\\max} \\, Q(\\delta(s,\\, a), \\, a')\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Where $P\\left(s' \\,| \\,s, a   \\right)$ is the probability that taking action $a$ in state $s$ will produce the next state $s'$. To implement that, we need to assume that the $\\hat{Q}$ values in our table may be wrong, due to the nondeterministic behavior of our environment. So, \\cite{Mitchell} suggested to modifying the training rule by introducing a decaying weighted average factor to the current $\\hat{Q}$ value and to the revised estimate, such that:\n",
    "\n",
    "$$\\hat{Q}(s, \\, a) \\leftarrow  (1 - \\alpha_{n})\\, \\hat{Q}_{n-1}(s, \\, a) +  \\alpha_{n}\\left[ r + \\gamma \\underset{a'}{\\max} \\hat{Q}_{n-1}(s', \\, a') \\right ]$$\n",
    "\n",
    "Where $\\alpha_{n} = \\frac{1}{1 + visits_{n}(s, \\,a)}$ is the decaying factor and $visits_{n}$ is the total number of times this state-action $(s, \\, a)$ pair has been visited. So, let's put it in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "_(approx. 2-3 pages)_\n",
    "\n",
    "### 4.1. Model Evaluation and Validation\n",
    "```\n",
    "Udacity:\n",
    "\n",
    "In this section, the final model and any supporting qualities should be evaluated in detail. It should be clear how the final model was derived and why this model was chosen. In addition, some type of analysis should be used to validate the robustness of this model and its solution, such as manipulating the input data or environment to see how the model’s solution is affected (this is called sensitivity analysis). Questions to ask yourself when writing this section:\n",
    "- Is the final model reasonable and aligning with solution expectations? Are the final parameters of the model appropriate?\n",
    "- Has the final model been tested with various inputs to evaluate whether the model generalizes well to unseen data?\n",
    "- Is the model robust enough for the problem? Do small perturbations (changes) in training data or the input space greatly affect the results?\n",
    "- Can results found from the model be trusted?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.2. Justification\n",
    "```\n",
    "Udacity:\n",
    "\n",
    "In this section, your model’s final solution and its results should be compared to the benchmark you established earlier in the project using some type of statistical analysis. You should also justify whether these results and the solution are significant enough to have solved the problem posed in the project. Questions to ask yourself when writing this section:\n",
    "- Are the final results found stronger than the benchmark result reported earlier?\n",
    "- Have you thoroughly analyzed and discussed the final solution?\n",
    "- Is the final solution significant enough to have solved the problem?\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "_(approx. 1-2 pages)_\n",
    "\n",
    "### 5.1. Free-Form Visualization\n",
    "```\n",
    "Udacity:\n",
    "\n",
    "In this section, you will need to provide some form of visualization that emphasizes an important quality about the project. It is much more free-form, but should reasonably support a significant result or characteristic about the problem that you want to discuss. Questions to ask yourself when writing this section:\n",
    "- Have you visualized a relevant or important quality about the problem, dataset, input data, or results?\n",
    "- Is the visualization thoroughly analyzed and discussed?\n",
    "- If a plot is provided, are the axes, title, and datum clearly defined?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5.2. Reflection\n",
    "```\n",
    "Udacity:\n",
    "\n",
    "In this section, you will summarize the entire end-to-end problem solution and discuss one or two particular aspects of the project you found interesting or difficult. You are expected to reflect on the project as a whole to show that you have a firm understanding of the entire process employed in your work. Questions to ask yourself when writing this section:\n",
    "- Have you thoroughly summarized the entire process you used for this project?\n",
    "- Were there any interesting aspects of the project?\n",
    "- Were there any difficult aspects of the project?\n",
    "- Does the final model and solution fit your expectations for the problem, and should it be used in a general setting to solve these types of problems?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Improvement\n",
    "```\n",
    "Udacity:\n",
    "\n",
    "In this section, you will need to provide discussion as to how one aspect of the implementation you designed could be improved. As an example, consider ways your implementation can be made more general, and what would need to be modified. You do not need to make this improvement, but the potential solutions resulting from these changes are considered and compared/contrasted to your current solution. Questions to ask yourself when writing this section:\n",
    "- Are there further improvements that could be made on the algorithms or techniques you used in this project?\n",
    "- Were there algorithms or techniques you researched that you did not know how to implement, but would consider using if you knew how?\n",
    "- If you used your final solution as the new benchmark, do you think an even better solution exists?\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Style notebook and change matplotlib defaults*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width:800px;\n",
       "        margin-left:16% !important;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width:800px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "    .prompt{\n",
       "        display: None;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 22pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "<style>\n",
       "    table {\n",
       "        overflow:hidden;\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        font-size: 12px;\n",
       "        margin: 10px;\n",
       "        width: 480px;\n",
       "        text-align: left;\n",
       "        border-collapse: collapse;\n",
       "        border: 1px solid #d3d3d3;\n",
       "        -moz-border-radius:5px; FF1+;\n",
       "        -webkit-border-radius:5px; Saf3-4;\n",
       "        border-radius:5px;\n",
       "        -moz-box-shadow: 0 0 4px rgba(0, 0, 0, 0.01);\n",
       "    }\n",
       "    th\n",
       "    {\n",
       "        padding: 12px 17px 12px 17px;\n",
       "        font-weight: normal;\n",
       "        font-size: 14px;\n",
       "        border-bottom: 1px dashed #69c;\n",
       "    }\n",
       "\n",
       "    td\n",
       "    {\n",
       "        padding: 7px 17px 7px 17px;\n",
       "\n",
       "    }\n",
       "\n",
       "    tbody tr:hover th\n",
       "    {\n",
       "\n",
       "        background:  #E9E9E9;\n",
       "    }\n",
       "\n",
       "    tbody tr:hover td\n",
       "    {\n",
       "\n",
       "        background:  #E9E9E9;\n",
       "    }\n",
       "\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading style sheet\n",
    "from IPython.core.display import HTML\n",
    "HTML( open('ipython_style.css').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#changing matplotlib defaults\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_palette(\"deep\", desat=.6)\n",
    "sns.set_context(rc={\"figure.figsize\": (8, 4)})\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(sns.color_palette(\"Set2\", 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
