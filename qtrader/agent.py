#!/usr/bin/python
# -*- coding: utf-8 -*-
"""
Implement and run an agent to learn in reinforcement learning framework

@author: ucaiado

Created on 08/18/2016
"""
import random
from environment import Agent, Environment
from simulator import Simulator
import logging
import sys
import time
from bintrees import FastRBTree
from collections import defaultdict
import pandas as pd

# Log finle enabled. global variable
DEBUG = False


# setup logging messages
if DEBUG:
    s_format = '%(asctime)s;%(message)s'
    s_now = time.strftime('%c')
    s_now = s_now.replace('/', '').replace(' ', '_').replace(':', '')
    s_file = 'log/sim_{}.log'.format(s_now)
    logging.basicConfig(filename=s_file, format=s_format)
    root = logging.getLogger()
    root.setLevel(logging.DEBUG)
    ch = logging.StreamHandler(sys.stdout)
    ch.setLevel(logging.DEBUG)

    formatter = logging.Formatter(s_format)
    ch.setFormatter(formatter)
    root.addHandler(ch)


'''
Begin help functions
'''


def save_q_table(e):
    '''
    Log the final Q-table of the algorithm
    :param e: Environment object. The grid-like world
    '''
    agent = e.primary_agent
    try:
        q_table = agent.q_table
        pd.DataFrame(q_table).T.to_csv('log/qtable.log', sep='\t')
    except:
        print 'No Q-table to be printed'

'''
End help functions
'''


class BasicAgent(Agent):
    '''
    A Basic agent representation that learns to drive in the smartcab world.
    '''
    def __init__(self, env, i_id, f_min_time=1.):
        '''
        Initiate a BasicAgent object. save all parameters as attributes
        :param env: Environment Object. The Environment where the agent acts
        :param i_id: integer. Agent id
        :param f_min_time: float. Minimum time in seconds to the agent react
        '''
        # sets self.env = env
        super(BasicAgent, self).__init__(env, i_id)
        # Initialize any additional variables here
        self.f_min_time = f_min_time
        self.next_time = 0.

    def reset(self):
        '''
        Reset the state and the agent's memory about its positions
        '''
        self.state = None
        self.position = {'qAsk': 0, 'Ask': 0., 'qBid': 0, 'Bid': 0.}
        self.d_order_tree = {'BID': FastRBTree(), 'ASK': FastRBTree()}
        self.d_order_map = {}
        # Reset any variables here, if required

    def update(self, s_msg):
        '''
        Update the state of the agent
        :param s_msg: string. A message generated by the order matching
        :param i_time: integer. The current time of the order book
        '''
        # check if should update the agent. If the environment message is not
        # a trade, the agent will update just if has passed enough time
        if s_msg['order_status'] not in ['Filled', 'Partially Filled']:
            if self.env.order_matching.my_book.f_time < self.next_time:
                return None

        # recover basic infos
        inputs = self.env.sense(self)
        state = self.env.agent_states[self]

        # Update state (position ,volume and if has an order in bid or ask)
        self.state = self._get_intern_state(inputs, state)

        # Select action according to the agent's policy
        # action = self._take_action(self.state, s_msg)
        action = None

        # # Execute action and get reward
        # print '\ncurrent action: {}\n'.format(action)
        reward = self.env.act(self, action)

        # Learn policy based on state, action, reward
        self._apply_policy(self.state, action, reward)
        # calculate the next time that the agent will react
        self.next_time = self.env.order_matching.my_book.f_time
        self.next_time += self.f_min_time

        # [debug]
        s_rtn = 'BasicAgent.update(): position = {}, inputs = {}, action'
        s_rtn += ' = {}, reward = {}'
        if DEBUG:
            root.debug(s_rtn.format(state['Position'],
                                    inputs,
                                    action,
                                    reward))
        else:
            print s_rtn.format(state['Position'],
                               inputs,
                               action['action'],
                               reward)

    def _get_intern_state(self, inputs, position):
        '''
        Return a tuple representing the intern state of the agent
        :param inputs: dictionary. traffic light and presence of cars
        :param position: dictionary. the current position of the agent
        '''
        pass

    def _take_action(self, t_state, s_msg):
        '''
        Return an action according to the agent policy
        :param t_state: tuple. The inputs to be considered by the agent
        :param s_msg: string. Order matching message
        '''
        return s_msg

    def _apply_policy(self, state, action, reward):
        '''
        Learn policy based on state, action, reward
        :param state: dictionary. The current state of the agent
        :param action: string. the action selected at this time
        :param reward: integer. the rewards received due to the action
        '''
        pass


def run():
    """
    Run the agent for a finite number of trials.
    """
    # Set up environment and agent
    e = Environment()  # create environment (also adds some dummy traffic)
    a = e.create_agent(BasicAgent)  # create agent
    e.set_primary_agent(a)  # specify agent to track

    # Now simulate it
    sim = Simulator(e, update_delay=1.00, display=False)
    sim.run(n_trials=1)  # run for a specified number of trials

    # save the Q table of the primary agent
    # save_q_table(e)


if __name__ == '__main__':
    # run the code
    run()
